{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasrc_home = \"./datasrc\"\n",
    "md5sums_file = \"./datasrc/MD5SUMS\"\n",
    "\n",
    "## Library Functions\n",
    "\n",
    "def md5(fname):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    try:\n",
    "        with open(fname, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    \n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset's resource list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset's resource list:\n",
    "# - Docentes da Pós-Graduação Stricto Sensu no Brasil de 2013 a 2016\n",
    "# - Docentes da Pós-Graduação Stricto Sensu no Brasil 2017\n",
    "# - Programas da Pós-Graduação Stricto Sensu do Brasil de 2013 a 2016\n",
    "# - Programas da Pós-Graduação Stricto Sensu no Brasil 2017\n",
    "\n",
    "# Fields: ds_name, ds_hash, resource_name, resource_hash, rfilename, download\n",
    "#table = [[\"Docentes 2013 a 2016\" , \"35eab2f8-5a64-4619-b3f1-63a2e6690cfa\", \"2013\", \"3f5c3276-ff3a-496c-9250-b2cf87879e1f\", \"br-capes-colsucup-docente-2013a2016-2017-12-02_2013.csv\", False],\n",
    "         #[\"Docentes 2013 a 2016\" , \"35eab2f8-5a64-4619-b3f1-63a2e6690cfa\", \"2014\", \"0bd87bca-8202-4404-8628-73c92f29721d\", \"br-capes-colsucup-docente-2013a2016-2017-12-02_2014.csv\", False],\n",
    "         #[\"Docentes 2013 a 2016\" , \"35eab2f8-5a64-4619-b3f1-63a2e6690cfa\", \"2015\", \"75eea9d5-1542-4cfd-8ed9-d540d3eef344\", \"br-capes-colsucup-docente-2013a2016-2017-12-02_2015.csv\", False],\n",
    "         #[\"Docentes 2013 a 2016\" , \"35eab2f8-5a64-4619-b3f1-63a2e6690cfa\", \"2016\", \"922bc0d1-90eb-4939-9167-03831f732f72\", \"br-capes-colsucup-docente-2013a2016-2017-12-02_2016.csv\", False],\n",
    "         #[\"Docentes 2017\"        , \"57f86b23-e751-4834-8537-e9d33bd608b6\", \"2017\", \"d918d02e-7180-4c7c-be73-980f9a8c09b5\", \"br-capes-colsucup-docente-2017-2018-08-10.csv\", False],\n",
    "         #[\"Programas 2013 a 2016\", \"122620f6-47dc-4363-9d63-130c8a386af6\", \"2013\", \"7de14e9c-9739-43d9-8217-ba9bf837b411\", \"br-capes-colsucup-prog-2013a2016-2017-12-02_2013.csv\", False],\n",
    "         #[\"Programas 2013 a 2016\", \"122620f6-47dc-4363-9d63-130c8a386af6\", \"2014\", \"a0c1760a-4130-49b7-b1fd-849ca189417b\", \"br-capes-colsucup-prog-2013a2016-2017-12-02_2014.csv\", False],\n",
    "table =  [[\"Programas 2013 a 2016\", \"122620f6-47dc-4363-9d63-130c8a386af6\", \"2015\", \"3c16cfcf-0614-4497-a3d4-324c0788fe2e\", \"br-capes-colsucup-prog-2013a2016-2017-12-02_2015.csv\", False],\n",
    "         [\"Programas 2013 a 2016\", \"122620f6-47dc-4363-9d63-130c8a386af6\", \"2016\", \"bc2fb7a9-8313-4959-abee-14764d812e8b\", \"br-capes-colsucup-prog-2013a2016-2017-12-02_2016.csv\", False],\n",
    "         [\"Programas 2017\"       , \"903b4215-ea91-4927-8975-d1484891374f\", \"2017\", \"8b3464e2-9108-4855-bc5b-2df474fdf152\", \"br-capes-colsucup-prog-2017-2018-08-01.csv\", False]]\n",
    "\n",
    "# example url: https://dadosabertos.capes.gov.br/dataset/35eab2f8-5a64-4619-b3f1-63a2e6690cfa/resource/3f5c3276-ff3a-496c-9250-b2cf87879e1f/download/br-capes-colsucup-docente-2013a2016-2017-12-02_2013.csv\n",
    "df = pd.DataFrame(table, columns = 'ds_name ds_hash resource_name resource_hash rfilename download'.split())\n",
    "df['url'] = 'https://dadosabertos.capes.gov.br/dataset/' + df['ds_hash'] + \\\n",
    "            '/resource/' + df['resource_hash'] + \\\n",
    "            \"/download/\" + df['rfilename']\n",
    "df['filename'] = df['rfilename'].apply(lambda x: x.split(sep='.')[0]) #filename without extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check MD5SUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5SUMS file found! \n"
     ]
    }
   ],
   "source": [
    "## Check if CSV file exists and is not corrupted\n",
    "\n",
    "# Read MD5SUMS\n",
    "try:    \n",
    "    with open(md5sums_file, 'r') as f:\n",
    "        print(\"MD5SUMS file found! \")\n",
    "        \n",
    "        # read md5sum['md5sum', 'lfilename']\n",
    "        md5sums = pd.read_csv(md5sums_file, sep=' ', header=None, names=['md5sum', 'lfilename'])\n",
    "\n",
    "        # add md5sum['filename']\n",
    "        md5sums['filename'] = md5sums['lfilename'].apply(lambda x: x.split(sep='.')[0]) #filename without extension\n",
    "        \n",
    "        # Compute MD5SUM of existing files\n",
    "        # add md5sum['check']\n",
    "        md5sums['check'] = md5sums['lfilename'].apply(lambda x: md5(datasrc_home + '/' + x))\n",
    "        \n",
    "        # add md5sum['corrupted']\n",
    "        md5sums['corrupted'] = (md5sums['md5sum'] == md5sums['check']).apply(lambda x: not(x))\n",
    "        \n",
    "        # merge md5sums with df\n",
    "        df = (pd.merge(df, md5sums[['corrupted', 'lfilename', 'filename', 'md5sum']], how='left', on='filename'))\n",
    "        \n",
    "        # set files to be downloaded - df['download']\n",
    "        df ['download'] = (df['download'] | df['corrupted'])        \n",
    "        \n",
    "except FileNotFoundError:\n",
    "\n",
    "    print(\"MD5SUMS file not found. The whole dataset will be downloaded.\")\n",
    "    \n",
    "    # set download to true\n",
    "    df['download'] = df['download'].apply(lambda x: True)\n",
    "    \n",
    "    # add df['lfilename']\n",
    "    df['lfilename'] = df['filename'] + \".csv.gz\"\n",
    "    \n",
    "    # set df['md5sum']\n",
    "    df['md5sum'] = df['filename'].apply(lambda x: 0)\n",
    "\n",
    "# Clenup df\n",
    "df = df[['filename', 'lfilename', 'md5sum', 'url', 'download']]\n",
    "\n",
    "# Define CSV files to be retrieved\n",
    "to_retrieve = df[df['download'] == True]['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to download 0 datasets.\n"
     ]
    }
   ],
   "source": [
    "# Start ds retrieval\n",
    "print(\"Need to download {} datasets.\".format(len(to_retrieve)))\n",
    "#for i in range(len(to_retrieve)):\n",
    "for i in df[df['download'] == True].index:\n",
    "    print('Retriveing \"{}\". '.format(df['filename'][i]), end=\"\")\n",
    "    \n",
    "    resource_df = pd.read_csv(df['url'][i], index_col=17, sep=';',encoding='iso-8859-1')\n",
    "    resource_df.to_csv(str(datasrc_home + \"/\" + df['lfilename'][i]), sep=';', compression='gzip')\n",
    "    \n",
    "    print('ok.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update MD5SUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. \"./datasrc/MD5SUMS\" written\n"
     ]
    }
   ],
   "source": [
    "# Compute new file's md5sum\n",
    "df['md5sum'] = df['lfilename'].apply(lambda x: md5(datasrc_home + '/' + x))\n",
    "\n",
    "# Write MD5SUMS\n",
    "try:    \n",
    "    with open(md5sums_file, 'w') as f:\n",
    "        df[['md5sum', 'lfilename']].to_csv(md5sums_file, sep=' ', header=None, index=False)\n",
    "        print('Success. \"{}\" written'.format(md5sums_file))\n",
    "        \n",
    "except FileNotFoundError:\n",
    "\n",
    "    print('Error. Could open \"{}\" for writing'.format(md5sums_file))\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
